<!DOCTYPE html>
<html lang="en"><head><title>Language Models for Text Classification: Is In-Context Learning Enough?</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inconsolata&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Rubik:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Language Models for Text Classification: Is In-Context Learning Enough?"/><meta property="og:description" content="Link: [2403.17661] Language Models for Text Classification: Is In-Context Learning Enough? This paper explores how text generation models combined with prompting techniques ..."/><meta property="og:image" content="https://akash7243.github.io/homepage/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../static/icon.png"/><meta name="description" content="Link: [2403.17661] Language Models for Text Classification: Is In-Context Learning Enough? This paper explores how text generation models combined with prompting techniques ..."/><meta name="generator" content="Quartz"/><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"></div><div class="center"><div class="page-header"><header><div class="explorer mobile-only"><button type="button" id="explorer" class="collapsed" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-tree="[{&quot;path&quot;:&quot;Daily-Notes&quot;,&quot;collapsed&quot;:true},{&quot;path&quot;:&quot;Paper-Reading&quot;,&quot;collapsed&quot;:true}]" data-mobileonly="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><div id="explorer-content"><ul class="overflow" id="explorer-ul"><li><div class="folder-outer open"><ul style="padding-left:0;" class="content" data-folderul><li><div class="folder-outer "><ul style="padding-left:0;" class="content" data-folderul></ul></div></li><li><div class="folder-container"><div data-folderpath="Daily-Notes"><a href="../Daily-Notes" data-for="Daily-Notes" class="folder-title">Daily Notes</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Daily-Notes"><li><a href="../Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse" data-for="Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse">Linear Decoding and Deep Nets with Neural Collapse</a></li><li><a href="../Daily-Notes/Masked-Language-Modeling" data-for="Daily-Notes/Masked-Language-Modeling">Masked Language Modeling</a></li><li><a href="../Daily-Notes/Toy-Models-of-Superposition" data-for="Daily-Notes/Toy-Models-of-Superposition">Notes on Toy Models of Superposition</a></li><li><a href="../Daily-Notes/Some-definitions-in-Information-Theory" data-for="Daily-Notes/Some-definitions-in-Information-Theory">Some definitions in Information Theory</a></li></ul></div></li><li><div class="folder-container"><div data-folderpath="Paper-Reading"><a href="../Paper-Reading" data-for="Paper-Reading" class="folder-title">Paper Reading</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Paper-Reading"><li><a href="../Paper-Reading/How-Does-IB-help-DL" data-for="Paper-Reading/How-Does-IB-help-DL">How Does Information Bottleneck Help Deep Learning?</a></li><li><a href="../Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough" data-for="Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough">Language Models for Text Classification: Is In-Context Learning Enough?</a></li><li><a href="../Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge" data-for="Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge">LLMs Struggle to Learn Long-Tail Knowledge</a></li></ul></div></li></ul></div></li><li id="explorer-end"></li></ul></div></div><h1 class="page-title"><a href="..">Akash Sharma</a></h1><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></header><div class="popover-hint"><h1 class="article-title">Language Models for Text Classification: Is In-Context Learning Enough?</h1><p show-comma="true" class="content-meta"><span>Jun 29, 2024</span><span>2 min read</span></p><ul class="tags"><li><a href="../tags/llms" class="internal tag-link">llms</a></li><li><a href="../tags/incontextlearning" class="internal tag-link">incontextlearning</a></li><li><a href="../tags/textclassification" class="internal tag-link">textclassification</a></li></ul></div></div><article class="popover-hint"><p><strong>Link:</strong> <a href="https://arxiv.org/abs/2403.17661" class="external">[2403.17661] Language Models for Text Classification: Is In-Context Learning Enough?<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p>This paper explores how text generation models combined with prompting techniques (in-context learning) compare to fine-tuning for text classification i.e., the authors compare zero- and few-shot approaches of LLMs to fine-tuning smaller language models. <strong>They find that finetuning smaller and efficient (?) language models outperforms few-shot approaches in large language models for text classification.</strong></p>
<p>This note will be quite verbose and at times, verbatim, since this is the first time I am reading literature on this topic.</p>
<h1 id="prior-research">Prior Research<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#prior-research" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ol>
<li><strong>Standard approach for supervised text classification</strong> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span> Finetune LMs such as BERT using an additional classifier head but this requires a lot of data to achieve SOTA results and hence is not suitable for classifications tasks where class imbalances and data sparsity is present (data is generally hard to come by or is imbalanced in specific domains). More data, better performance!</li>
<li><strong>Alternative approach</strong> is based on using autoregressive text generation models which can perform unseen tasks through the use of prompting (they have zero and few-shot capabilities). In the zero-shot setting, instruction fine-tuning leads to improvement.</li>
<li>Prompt-engineering techniques have been used to improve generalization and for domain and task-specific improvements.</li>
</ol>
<h2 id="text-classification">Text Classification<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#text-classification" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>Three main approaches to text classification exist <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span></span></span></span></p>
<ol>
<li><strong>Linear Methods</strong>: FastText is a linear classification model which integrates a linear model with a rank constraint which allows sharing parameters among classes and features. It also integrates word embeddings which are average into text. <em>Advantages:</em> Dealing with out-of-vocabulary words and fine-grained distinctions between classes.</li>
<li><strong>Fine-tuning Methods</strong>: Models like BERT and RoBERTa trained with masked language modeling (MLM) objective can be adapted for text-classification by using fine-tuning techniques based on adding a single classification layer onto the model. As discussed above, it requires a lot of data.</li>
<li><strong>Text Generation Models</strong>:</li>
</ol>
<p><em>Continuing soon…</em></p>
<h1 id="appendix">Appendix<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#appendix" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<h2 id="masked-language-modeling-mlm">Masked Language Modeling (MLM)<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#masked-language-modeling-mlm" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>See <a href="../Daily-Notes/Masked-Language-Modeling" class="internal alias" data-slug="Daily-Notes/Masked-Language-Modeling">Masked Language Modeling</a> for more details.</p></article></div><div class="right sidebar"><div class="spacer desktop-only"></div><div class="spacer desktop-only"></div><div class="spacer desktop-only"></div><div class="spacer desktop-only"></div><div class="explorer desktop-only"><div id="explorer-content"><ul class="overflow" id="explorer-ul"><li><div class="folder-outer open"><ul style="padding-left:0;" class="content" data-folderul><li><div class="folder-outer "><ul style="padding-left:0;" class="content" data-folderul></ul></div></li><li><div class="folder-container"><div data-folderpath="Daily-Notes"><a href="../Daily-Notes" data-for="Daily-Notes" class="folder-title">Daily Notes</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Daily-Notes"><li><a href="../Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse" data-for="Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse">Linear Decoding and Deep Nets with Neural Collapse</a></li><li><a href="../Daily-Notes/Masked-Language-Modeling" data-for="Daily-Notes/Masked-Language-Modeling">Masked Language Modeling</a></li><li><a href="../Daily-Notes/Toy-Models-of-Superposition" data-for="Daily-Notes/Toy-Models-of-Superposition">Notes on Toy Models of Superposition</a></li><li><a href="../Daily-Notes/Some-definitions-in-Information-Theory" data-for="Daily-Notes/Some-definitions-in-Information-Theory">Some definitions in Information Theory</a></li></ul></div></li><li><div class="folder-container"><div data-folderpath="Paper-Reading"><a href="../Paper-Reading" data-for="Paper-Reading" class="folder-title">Paper Reading</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Paper-Reading"><li><a href="../Paper-Reading/How-Does-IB-help-DL" data-for="Paper-Reading/How-Does-IB-help-DL">How Does Information Bottleneck Help Deep Learning?</a></li><li><a href="../Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough" data-for="Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough">Language Models for Text Classification: Is In-Context Learning Enough?</a></li><li><a href="../Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge" data-for="Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge">LLMs Struggle to Learn Long-Tail Knowledge</a></li></ul></div></li></ul></div></li><li id="explorer-end"></li></ul></div></div><div class="spacer desktop-only"></div><div class="toc"><button type="button" id="toc" class="collapsed"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#prior-research" data-for="prior-research">Prior Research</a></li><li class="depth-1"><a href="#text-classification" data-for="text-classification">Text Classification</a></li><li class="depth-0"><a href="#appendix" data-for="appendix">Appendix</a></li><li class="depth-1"><a href="#masked-language-modeling-mlm" data-for="masked-language-modeling-mlm">Masked Language Modeling (MLM)</a></li></ul></div></div><div class="spacer desktop-only"></div></div></div><footer class><hr/><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> © 2024</p><ul><li><a href="https://github.com/akashrma">GitHub</a></li><li><a href="https://twitter.com/mathcrush247">Twitter</a></li><li><a href="https://www.linkedin.com/in/akashsharma7243/">LinkedIn</a></li></ul></footer></div></body><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script type="application/javascript">
            const socket = new WebSocket('ws://localhost:3001')
            // reload(true) ensures resources like images and scripts are fetched again in firefox
            socket.addEventListener('message', () => document.location.reload(true))
          </script><script src="../postscript.js" type="module"></script></html>