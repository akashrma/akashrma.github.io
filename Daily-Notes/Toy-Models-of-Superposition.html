<!DOCTYPE html>
<html lang="en"><head><title>Notes on Toy Models of Superposition</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inconsolata&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Rubik:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="Notes on Toy Models of Superposition"/><meta property="og:description" content="Notes from the original paper Toy models used — small ReLU networks trained on synthetic data with sparse input features. What do they mean by “sparse” features? Most of the values in the feature space are zero."/><meta property="og:image" content="https://akash7243.github.io/homepage/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="../static/icon.png"/><meta name="description" content="Notes from the original paper Toy models used — small ReLU networks trained on synthetic data with sparse input features. What do they mean by “sparse” features? Most of the values in the feature space are zero."/><meta name="generator" content="Quartz"/><link href="../index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Daily-Notes/Toy-Models-of-Superposition"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"></div><div class="center"><div class="page-header"><header><div class="explorer mobile-only"><button type="button" id="explorer" class="collapsed" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-tree="[{&quot;path&quot;:&quot;Daily-Notes&quot;,&quot;collapsed&quot;:true},{&quot;path&quot;:&quot;Paper-Reading&quot;,&quot;collapsed&quot;:true}]" data-mobileonly="true"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><div id="explorer-content"><ul class="overflow" id="explorer-ul"><li><div class="folder-outer open"><ul style="padding-left:0;" class="content" data-folderul><li><div class="folder-outer "><ul style="padding-left:0;" class="content" data-folderul></ul></div></li><li><div class="folder-container"><div data-folderpath="Daily-Notes"><a href="../Daily-Notes" data-for="Daily-Notes" class="folder-title">Daily Notes</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Daily-Notes"><li><a href="../Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse" data-for="Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse">Linear Decoding and Deep Nets with Neural Collapse</a></li><li><a href="../Daily-Notes/Masked-Language-Modeling" data-for="Daily-Notes/Masked-Language-Modeling">Masked Language Modeling</a></li><li><a href="../Daily-Notes/Toy-Models-of-Superposition" data-for="Daily-Notes/Toy-Models-of-Superposition">Notes on Toy Models of Superposition</a></li><li><a href="../Daily-Notes/Some-definitions-in-Information-Theory" data-for="Daily-Notes/Some-definitions-in-Information-Theory">Some definitions in Information Theory</a></li></ul></div></li><li><div class="folder-container"><div data-folderpath="Paper-Reading"><a href="../Paper-Reading" data-for="Paper-Reading" class="folder-title">Paper Reading</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Paper-Reading"><li><a href="../Paper-Reading/How-Does-IB-help-DL" data-for="Paper-Reading/How-Does-IB-help-DL">How Does Information Bottleneck Help Deep Learning?</a></li><li><a href="../Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough" data-for="Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough">Language Models for Text Classification: Is In-Context Learning Enough?</a></li><li><a href="../Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge" data-for="Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge">LLMs Struggle to Learn Long-Tail Knowledge</a></li></ul></div></li></ul></div></li><li id="explorer-end"></li></ul></div></div><h1 class="page-title"><a href="..">Akash Sharma</a></h1><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabindex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabindex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div></header><div class="popover-hint"><h1 class="article-title">Notes on Toy Models of Superposition</h1><p show-comma="true" class="content-meta"><span>Jul 16, 2024</span><span>4 min read</span></p></div></div><article class="popover-hint"><h1 id="notes-from-the-original-paper">Notes from the original paper<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#notes-from-the-original-paper" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p><strong>Toy models used</strong> — small ReLU networks trained on synthetic data with <em>sparse</em> input features.</p>
<blockquote class="callout question" data-callout="question">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>What do they mean by “sparse” features? </p></div>
                  
                </div>
</blockquote>
<p>Most of the values in the feature space are zero.</p>
<blockquote class="callout question" data-callout="question">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Why? </p></div>
                  
                </div>
<p>To investigate how and when models represent more features than they have dimensions — this is superposition.</p>
</blockquote>
<ul>
<li><strong>Compression</strong>. When features are sparse, superposition allows compression beyond what a linear model would do but at the cost of “interference” that requires nonlinear filtering.</li>
</ul>
<blockquote class="callout example" data-callout="example">
<div class="callout-title">
                  <div class="callout-icon"></div>
                  <div class="callout-title-inner"><p>Example</p></div>
                  
                </div>
<p>Consider a toy model where we - train an embedding of five features of varying importance (scalar multiplier on MSE loss) in 2D, add ReLU afterwards for filtering and vary sparsity of the features.</p>
</blockquote>
<p><strong><img src="https://lh7-us.googleusercontent.com/docsz/AD_4nXdYRf8HhoOzj91GYmSsXXxfb7jVtf6mgBsU7ytSIaiDJctRf9QOgq5jTpsMWcXIvITRd5zvUWC98rYXqgPI_qZzuYFITEju5Dlfdh0jaJgqRbFsSndOmvev3C352rX0BIqgWW_VIeBrzQg6nqLfdAb5rbly?key=AkOUFyUFEoYJHEC7N0_mkA" alt/></strong></p>
<p><strong>Observations</strong></p>
<ul>
<li>If the features are dense, the model learns to represent an orthogonal basis of the most important two features (similar to PCA), others are mapped to 0. As the sparsity increases, the lesser important features also start to appear in the embedding.</li>
<li>Here the number of dims = 2 but the features represented are more than in case of higher sparsity, models use “superposition” here.</li>
<li>However, there might be some interference as shown in 90% sparsity case — basically, a feature representation will have a non-zero component along another feature representation.</li>
</ul>
<p><strong>Models can perform computation while in superposition</strong>.</p>
<ul>
<li>Models can put simple circuits computing the absolute value function in superposition. (What?)</li>
<li><strong>Hypothesis</strong>: Neural networks observed in practice noisily simulate larger, highly sparse networks. — Models can be thought of as doing the same thing as an imagined much-larger model representing the exact same features but with no interference.</li>
</ul>
<p><strong>Feature superposition in other places! (Read later)</strong></p>
<ul>
<li>Linear Algebraic structure of word sense, with applications to polysemy (TACL 2018, Arora et. al)</li>
<li>Decoding the Thought Vector (2016, G. Goh)</li>
<li>Zoom In: An Introduction to Circuits (Distill 2020, Olah et. al)</li>
<li>Softmax Linear Units (Transformer Circuits Thread 2022, Elhage et. al)</li>
<li>Compressed Sensing (IEEE TIT 2006, Donoho et. al.)</li>
<li>Local vs. Distributed Coding (Intellectica 1989, Thorpe)</li>
<li>Representation learning: A review and new perspectives (IEEE TPAMI 2013, Bengio et. al.)</li>
</ul>
<p><strong>What do they show in this paper?</strong></p>
<ul>
<li>Interpreting NNs as having sparse structure in superposition is not only a useful post-hoc interpretation but the “ground truth” of the model.</li>
<li><em>When and why</em>? → “phase” diagram for superposition.
<ul>
<li>Why neurons are sometimes monosemantic and sometimes polysemantic?
<ul>
<li>Monosemantic → respond to a single feature.</li>
<li>Polysemantic → respond to multiple features.</li>
</ul>
</li>
</ul>
</li>
<li>Superposition exhibits complex geometric structure based uniform polytopes.
<ul>
<li><strong>Uniform polytopes</strong>
<ul>
<li>Class of regular polytopes in any number of dimensions. Think regular polygons in 2D, regular polyhedra in 3D.</li>
<li>*Conditions for uniformity
<ul>
<li><strong>Vertex-transitivity</strong>: all vertices are equivalent under the symmetries of the polytope.</li>
<li><strong>Edge-transitivity</strong>: all edges are equivalent.</li>
<li><strong>Face-transitivity</strong>: all facets (faces) are equivalent.</li>
<li><strong>Vertex figures are regular</strong>: Figure formed by the intersection of the polytope with its facets must be a regular polytope of one dim lower.</li>
</ul>
</li>
</ul>
</li>
<li>Preliminary evidence that superposition may be linked to
<ul>
<li>adversarial examples</li>
<li>grokking (?)</li>
<li>theory for the performance of mixture of experts model.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Definitions and Motivations: Features, Directions and Superposition</strong></p>
<ul>
<li>They think of features of inputs as directions in activation space. Like in the example above — there are 2 dim in activation space and 5 features in the input.</li>
<li><strong>“Linear Representation Hypothesis”.</strong>  Two separate properties
<ul>
<li><strong>Decomposability</strong> →  Network representations can be described in terms of independently understandable features.</li>
<li><strong>Linearity</strong> → Features are represented by direction.</li>
</ul>
</li>
</ul>
<h1 id="notes-from-neel-nandas-walkthrough-video">Notes from Neel Nanda’s Walkthrough Video<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#notes-from-neel-nandas-walkthrough-video" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ul>
<li>High-dimensional spaces are not intuitive to understand.
<ul>
<li>It is easier to fit a lot of features (nearly orthogonal) in very high-dimensional spaces.</li>
</ul>
</li>
<li><strong>Interference vs. Internal Representation</strong>. The linear map encoder projects input into 2 dimensions and read-out using dot product (basically projecting into some direction) → from 2-dim to 5-dim. Key point is that encoding and decoding are not inverse operations. Project down operation is not inverse of embed up.</li>
<li><strong>Superposition in Toy models vs. Real transformers</strong>. Discussion on where transformer might be compressing. Residual stream has much lower dimensions (bottleneck dimensions). Non-linearities cause more interference.</li>
<li><strong>About non-privileged basis</strong> <span>→</span>
<ul>
<li>Features are not invariant under a change of basis.
<ul>
<li><span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> <span>→</span> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> will change the internal representations.</li>
<li>This is a non-privileged basis.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Some weird stuff!</strong>
<ul>
<li>Neel says that neural networks in computers are computational objects not abstract linear algebra objects. So in computers, the way you represent vectors as floats in a computer is always privileged.</li>
<li>Floating points represented in a computer — 32 bits (MSB - signed bit (+1 or -1), floating point representation in binary, exponent bit, another binary string).</li>
</ul>
</li>
</ul>
<h1 id="references">References<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#references" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>[1] Elhage, et al., “Toy Models of Superposition”, Transformer Circuits Thread, 2022.</p>
<p>[2] <a href="https://www.youtube.com/watch?v=R3nbXgMnVqQ" class="external">A Walkthrough of Toy Models of Superposition w/ Jess Smith - YouTube<svg class="external-icon" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p></article></div><div class="right sidebar"><div class="spacer desktop-only"></div><div class="spacer desktop-only"></div><div class="spacer desktop-only"></div><div class="spacer desktop-only"></div><div class="explorer desktop-only"><div id="explorer-content"><ul class="overflow" id="explorer-ul"><li><div class="folder-outer open"><ul style="padding-left:0;" class="content" data-folderul><li><div class="folder-outer "><ul style="padding-left:0;" class="content" data-folderul></ul></div></li><li><div class="folder-container"><div data-folderpath="Daily-Notes"><a href="../Daily-Notes" data-for="Daily-Notes" class="folder-title">Daily Notes</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Daily-Notes"><li><a href="../Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse" data-for="Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse">Linear Decoding and Deep Nets with Neural Collapse</a></li><li><a href="../Daily-Notes/Masked-Language-Modeling" data-for="Daily-Notes/Masked-Language-Modeling">Masked Language Modeling</a></li><li><a href="../Daily-Notes/Toy-Models-of-Superposition" data-for="Daily-Notes/Toy-Models-of-Superposition">Notes on Toy Models of Superposition</a></li><li><a href="../Daily-Notes/Some-definitions-in-Information-Theory" data-for="Daily-Notes/Some-definitions-in-Information-Theory">Some definitions in Information Theory</a></li></ul></div></li><li><div class="folder-container"><div data-folderpath="Paper-Reading"><a href="../Paper-Reading" data-for="Paper-Reading" class="folder-title">Paper Reading</a></div></div><div class="folder-outer "><ul style="padding-left:1.4rem;" class="content" data-folderul="Paper-Reading"><li><a href="../Paper-Reading/How-Does-IB-help-DL" data-for="Paper-Reading/How-Does-IB-help-DL">How Does Information Bottleneck Help Deep Learning?</a></li><li><a href="../Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough" data-for="Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough">Language Models for Text Classification: Is In-Context Learning Enough?</a></li><li><a href="../Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge" data-for="Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge">LLMs Struggle to Learn Long-Tail Knowledge</a></li></ul></div></li></ul></div></li><li id="explorer-end"></li></ul></div></div><div class="spacer desktop-only"></div><div class="toc"><button type="button" id="toc" class="collapsed"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#notes-from-the-original-paper" data-for="notes-from-the-original-paper">Notes from the original paper</a></li><li class="depth-0"><a href="#notes-from-neel-nandas-walkthrough-video" data-for="notes-from-neel-nandas-walkthrough-video">Notes from Neel Nanda’s Walkthrough Video</a></li><li class="depth-0"><a href="#references" data-for="references">References</a></li></ul></div></div><div class="spacer desktop-only"></div></div></div><footer class><hr/><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.2.3</a> © 2024</p><ul><li><a href="https://github.com/akashrma">GitHub</a></li><li><a href="https://twitter.com/mathcrush247">Twitter</a></li><li><a href="https://www.linkedin.com/in/akashsharma7243/">LinkedIn</a></li></ul></footer></div></body><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/copy-tex.min.js" type="application/javascript"></script><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script type="module">
          let mermaidImport = undefined
          document.addEventListener('nav', async () => {
            if (document.querySelector("code.mermaid")) {
              mermaidImport ||= await import('https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs')
              const mermaid = mermaidImport.default
              const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
              mermaid.initialize({
                startOnLoad: false,
                securityLevel: 'loose',
                theme: darkMode ? 'dark' : 'default'
              })

              await mermaid.run({
                querySelector: '.mermaid'
              })
            }
          });
          </script><script type="application/javascript">
            const socket = new WebSocket('ws://localhost:3001')
            // reload(true) ensures resources like images and scripts are fetched again in firefox
            socket.addEventListener('message', () => document.location.reload(true))
          </script><script src="../postscript.js" type="module"></script></html>