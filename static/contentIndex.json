{"Daily-Notes/Some-definitions-in-Information-Theory":{"title":"Some definitions in Information Theory","links":[],"tags":["math"],"content":"\nEntropy can intuitively be thought of as a measure of information, for any probability distribution or in other words, uncertainty of a random variable.\nMutual information is the measure of the amount of information one random variable contains about another.\nRelative Entropy is the measure of the distance between two probability distributions.\n\n\n\n                  \n                  Note\n                  \n                \nEntropy can be thought of as the self-information of a random variable and mutual information is a special case of relative entropy.\n\nEntropy\n\n\n                  \n                  Definition\n                  \n                \nThe entropy H(X) of a discrete random variable X is defined by\nH(X)=−∑x∈X​p(x)logp(x)\n\nJoint Entropy\n\n\n                  \n                  Definition\n                  \n                \nThe joint entropy H(X,Y) of a pair of discrete random variables (X,Y) with a joint distribution p(x,y) is defined as\nH(X,Y)=−x∈X∑​y∈Y∑​p(x,y)logp(x,y)\nalso,\nH(X,Y)=−Elogp(X,Y)\n\nRelative Entropy or Kullback-Leibler Distance\n\n\n                  \n                  Definition\n                  \n                \nThe relative entropy or Kullback-Leibler distance between two probability mass functions p(x) and q(x) is defined as\nD(p∣∣q)​=x∈X∑​p(x)logq(x)p(x)​=Ep​logq(X)p(X)​​\n\nMutual Information\n\n\n                  \n                  Definition\n                  \n                \nConsider two random variables X and Y with a joint probability mass function p(x,y) and marginal probability mass functions p(x) and p(y). The mutual information I(X,Y) is the relative entropy between the joint distribution and the product distribution p(x)p(y):\nI(X;Y)​=x∈X∑​y∈Y∑​p(x,y)logp(x)p(y)p(x,y)​=D(p(x,y)∣∣p(x)p(y))=Ep(x,y)​logp(X)p(Y)p(X,Y)​​\n"},"Daily-Notes/index":{"title":"Daily Notes","links":[],"tags":[],"content":""},"index":{"title":"Akash Sharma","links":[],"tags":[],"content":"Akash SharmaMS (Research) Student, EE @ IIT Madras\nI’m currently a master’s graduate student at Department of Electrical Engineering at Indian Institute of Technology Madras. I am advised by Dr. Mohanasankar Sivaprakasam and Keerthi Ram. I am also a part of the analytics team at Sudha Gopalakrishnan Brain Centre, IIT Madras. My broad research interests include fundamental deep learning (theory and application) for both vision and language tasks.\nContact\nI am always excited to discuss/collaborate on anything ML, math and computing. The best way to reach out to me is via email but you can also find me on other platforms, please see bottom of this page."}}