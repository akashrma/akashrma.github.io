{"Daily-Notes/Some-definitions-in-Information-Theory":{"title":"Some definitions in Information Theory","links":[],"tags":["math"],"content":"\nEntropy can intuitively be thought of as a measure of information, for any probability distribution or in other words, uncertainty of a random variable.\nMutual information is the measure of the amount of information one random variable contains about another.\nRelative Entropy is the measure of the distance between two probability distributions.\n\n\n\n                  \n                  Note\n                  \n                \nEntropy can be thought of as the self-information of a random variable and mutual information is a special case of relative entropy.\n\nEntropy\n\n\n                  \n                  Definition\n                  \n                \nThe entropy H(X) of a discrete random variable X is defined by\nH(X)=−x∈X∑​p(x)logp(x)\n\nJoint Entropy\n\n\n                  \n                  Definition\n                  \n                \nThe joint entropy H(X,Y) of a pair of discrete random variables (X,Y) with a joint distribution p(x,y) is defined as\nH(X,Y)=−x∈X∑​y∈Y∑​p(x,y)logp(x,y)\nalso,\nH(X,Y)=−Elogp(X,Y)\n\nRelative Entropy or Kullback-Leibler Distance\n\n\n                  \n                  Definition\n                  \n                \nThe relative entropy or Kullback-Leibler distance between two probability mass functions p(x) and q(x) is defined as\nD(p∣∣q)​=x∈X∑​p(x)logq(x)p(x)​=Ep​logq(X)p(X)​​\n\nMutual Information\n\n\n                  \n                  Definition\n                  \n                \nConsider two random variables X and Y with a joint probability mass function p(x,y) and marginal probability mass functions p(x) and p(y). The mutual information I(X,Y) is the relative entropy between the joint distribution and the product distribution p(x)p(y):\nI(X;Y)​=x∈X∑​y∈Y∑​p(x,y)logp(x)p(y)p(x,y)​=D(p(x,y)∣∣p(x)p(y))=Ep(x,y)​logp(X)p(Y)p(X,Y)​​\n"},"Daily-Notes/index":{"title":"Daily Notes","links":[],"tags":[],"content":""},"Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge":{"title":"LLMs Struggle to Learn Long-Tail Knowledge","links":[],"tags":["llms","empirical"],"content":"In this paper, the authors show that a language model’s ability to answer fact-based questions is dependent on the related number of documents it pre-trained on. The general observation is that lesser the number of relevant (to the QA pair) documents the model pre-trained on, worse is its QA performance for that particular pair.\nMotivating Questions\n\nMany concepts or much knowledge appears rarely on the Internet (primary source for LLM training), are language models able to learn this knowledge well? How is the LLM’s ability to answer the question affected by the number of related documents seen during the pre-training?\nIf there is a correlation between the number of relevant documents and the QA performance, does it also mean causation?\nHow can such long-tail knowledge be better captured? How does model scaling and retrieval-augmentation affect performance on long-tail knowledge?\nWhat kind of knowledge language models capture − do they learn “easy” facts that appear more frequently in the pre-training data?\n\nHow?\n\n\n                  \n                  Question\n                  \n                \nHow do they find the relevant documents and count them?\nEntity linking the pre-training datasets and the counting documents with the same entities as the QA pair. DBpedia Spotlight Entity Linker is run at massively distributed scale and the entities are linked to DBpedia or Wikidata IDs using traditional entity linking methods. This lets us store the document indices for each entity.\nNext, the QA pair is entity linked and the entities from both the question and ground-truth answer is extracted. Once this is done, count the number of documents from the earlier pre-trained dataset entity linking for the extracted QA entities where both the question entity and the answer entity co-occur.\n\n\nReferences\n[1] Kandpal, Nikhil, et al. “Large language models struggle to learn long-tail knowledge.” International Conference on Machine Learning. PMLR, 2023."},"Paper-Reading/index":{"title":"Paper Reading","links":[],"tags":[],"content":""},"index":{"title":"Akash Sharma","links":[],"tags":[],"content":"Akash SharmaMS (Research) Student, EE @ IIT Madras\nI’m currently a master’s graduate student at Department of Electrical Engineering at Indian Institute of Technology Madras. I am advised by Dr. Mohanasankar Sivaprakasam and Keerthi Ram. I am also a part of the analytics team at Sudha Gopalakrishnan Brain Centre, IIT Madras. My broad research interests include fundamental deep learning (theory and application) for both vision and language tasks.\nContact\nI am always excited to discuss/collaborate on anything ML, math and computing. The best way to reach out to me is via email but you can also find me on other platforms, please see bottom of this page."}}