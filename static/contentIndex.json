{"Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse":{"title":"Linear Decoding and Deep Nets with Neural Collapse","links":[],"tags":["math","informationtheory"],"content":"XY Han, Prevalence of Neural Collapse during the terminal phase of deep learning train., 2021.10.05\nThe following notes are based on the seminar video by one of the original authors of the Neural Collapse paper.\nSimplex equiangular tight frame (ETF): Equinorm, Equiangle, Maximal Angle.\n[NC1] Variability collapse: within-class variable collapse (features collapse)\n[NC2] Class mean convergence to simplex ETF: within a space where number of vectors/classes is smaller than the dimension of the space. The trad ETF is within a setting a where there are more vectors than dimensions → the definition of the frame is that the smallest eigenvalue of the vectors of the frame is greater than zero, it spans the space. Here, the spanning condition is relaxed and the structure is retained.\n[NC3] Convergence to self-duality: means and classifier converge to each other. \n[NC4] Behavioral convergence to nearest class center: The classifier is simply the nearest neighbor to the class mean (See theoretical proof in the paper).\n\n\n                  \n                  What advantages do we have if we train beyond zero error? \n                  \n                \n\nImproves generalization, test performance.\nImproves adversarial robustness (DeepFool metric)\n\n\nFeature Engineering\nNotation exposes only last-layer objects: W and hi,c​=hi,cL​.\nFull notation (W,Θ)\n\nW : last-layer classifier (C-dimensional vectors)\nΘ : parameters of earlier layers\nLast-layer activations: \nhi,C​=h(Xi,c​;Θ)\n\nOptimizing h(⋅;Θ): ‘Feature engineering’.\n\n\n                  \n                  Neural Collapse is the preferred end-state of every successful exercise in feature engineering. Similar idea was proposed by Shannon in his 1959 paper “Probability of Error for Optimal Codes in a Gaussian Channel”.\n                  \n                \n\nLinear Decoding Over Gaussian Channel\nTransmitter transmits codeword: μγ​∈{μC​}c=1C​, γ∼Unif{1,...,C}.\nReceiver receives: h=μγ​+z, z∼N(0,σ2I).\nLinear decoder with weights  {wc​}c=1C​ and biases {bc​}c=1C​ :\nγ^​(h)=argcmax​⟨wc​,h⟩+bc​\nGoal: Design weights, biases and codewords such that ∣∣μc​∣∣2​⩽1. Here we have a norm bound on the means. So choose the best codeword so that the decoder’s job is the easiest, basically learn features in such a way that it is easier to classify as for our case.\nThis is similar to the feature engineering problem we have. Shannon gave bounds for what the angles between the codewords must be as the dimension of the codewords (size of the codewords) increases.\nFor us, we look at other things. We know that for deep nets, the signal overwhelms the noise therefore σ→0. So what is the limiting ratio that we should look at, limit exists as we go to infinity? Defined below. (this is different from what Shannon considered).\nLarge deviations error exponent →\nβ(H,W,b)=−σ2logPσ​{γ^​(h)=γ}\nwhere −σ2 and logPσ​{γ^​(h)=γ} are opposing as initially noise is high and signal is low, thus misclassification occurs and as we keep training σ lowers essentially tending to 0, first time thus increasing and the rate of misclassification decreases.\nOptimal Error Exponent →\nβC∗​≡H,W,bmax​β(H,W,b)\nReferences\n[1] Papyan, Vardan, X. Y. Han, and David L. Donoho. “Prevalence of neural collapse during the terminal phase of deep learning training.” Proceedings of the National Academy of Sciences 117.40 (2020): 24652-24663."},"Daily-Notes/Masked-Language-Modeling":{"title":"Masked Language Modeling","links":[],"tags":["llms","definition"],"content":"Setup\nWith BERT, the input sequence is a concatenation of two sequences of tokens (x1​,...,xN​) and (y1​,...,yM​), this input sequence is presented as follows:\n[CLS],x1​,...,xN​,[SEP],y1​,...,yM​,[EOS].\nM and N are subject to the constraint M+N&lt;T, where T is the maximal context/sequence length during training. The sequences are delimited by special tokens. The two sequences can be sampled contiguously from the same document with probability 0.5 or from distinct documents.\nTraining Objective\n\nPick a random sample of the tokens in the input sequence and replace it with a special token [MASK].\nMLM objective → Cross-Entropy Loss on predicting the masked tokens.\nFor BERT, the following happens −\n\nSelect 15% of the input tokens for possible replacement with uniform probability.\nOut of the selected tokens, replace 80%  with [MASK], leave 10% unchanged and replace the rest with a randomly selected token from the vocabulary.\n\n\n\nStatic vs. Dynamic Masking\nIn the RoBERTa paper, a dynamic masking strategy is used.\n\nStatic Masking: In the original implementation for BERT, masking is done during data preprocessing i.e., it is fixed during the training. This can lead to the same mask during each epoch for a specific sequence/training instance, so instead they duplicate the training set 10 times and mask it in 10 different ways. So, for a training instance, for 40 epochs, it is seen with 4 different masks during the training.\nDynamic Masking: Generate a masking pattern every time a sequence is fed to the model.\n\nDynamic masking is crucial when pre-training for more steps and larger datasets. It also performs comparably with static masking and is more efficient."},"Daily-Notes/Some-definitions-in-Information-Theory":{"title":"Some definitions in Information Theory","links":[],"tags":["math"],"content":"\nEntropy can intuitively be thought of as a measure of information, for any probability distribution or in other words, uncertainty of a random variable.\nMutual information is the measure of the amount of information one random variable contains about another.\nRelative Entropy is the measure of the distance between two probability distributions.\n\n\n\n                  \n                  Note\n                  \n                \nEntropy can be thought of as the self-information of a random variable and mutual information is a special case of relative entropy.\n\nEntropy\n\n\n                  \n                  Definition\n                  \n                \nThe entropy H(X) of a discrete random variable X is defined by\nH(X)=−x∈X∑​p(x)logp(x)\n\nJoint Entropy\n\n\n                  \n                  Definition\n                  \n                \nThe joint entropy H(X,Y) of a pair of discrete random variables (X,Y) with a joint distribution p(x,y) is defined as\nH(X,Y)=−x∈X∑​y∈Y∑​p(x,y)logp(x,y)\nalso,\nH(X,Y)=−Elogp(X,Y)\n\nRelative Entropy or Kullback-Leibler Distance\n\n\n                  \n                  Definition\n                  \n                \nThe relative entropy or Kullback-Leibler distance between two probability mass functions p(x) and q(x) is defined as\nD(p∣∣q)​=x∈X∑​p(x)logq(x)p(x)​=Ep​logq(X)p(X)​​\n\nMutual Information\n\n\n                  \n                  Definition\n                  \n                \nConsider two random variables X and Y with a joint probability mass function p(x,y) and marginal probability mass functions p(x) and p(y). The mutual information I(X,Y) is the relative entropy between the joint distribution and the product distribution p(x)p(y):\nI(X;Y)​=x∈X∑​y∈Y∑​p(x,y)logp(x)p(y)p(x,y)​=D(p(x,y)∣∣p(x)p(y))=Ep(x,y)​logp(X)p(Y)p(X,Y)​​\n"},"Daily-Notes/Toy-Models-of-Superposition":{"title":"Notes on Toy Models of Superposition","links":[],"tags":[],"content":"Notes from the original paper\nToy models used — small ReLU networks trained on synthetic data with sparse input features.\n\n\n                  \n                  What do they mean by “sparse” features? \n                  \n                \n\nMost of the values in the feature space are zero.\n\n\n                  \n                  Why? \n                  \n                \nTo investigate how and when models represent more features than they have dimensions — this is superposition.\n\n\nCompression. When features are sparse, superposition allows compression beyond what a linear model would do but at the cost of “interference” that requires nonlinear filtering.\n\n\n\n                  \n                  Example\n                  \n                \nConsider a toy model where we - train an embedding of five features of varying importance (scalar multiplier on MSE loss) in 2D, add ReLU afterwards for filtering and vary sparsity of the features.\n\n\nObservations\n\nIf the features are dense, the model learns to represent an orthogonal basis of the most important two features (similar to PCA), others are mapped to 0. As the sparsity increases, the lesser important features also start to appear in the embedding.\nHere the number of dims = 2 but the features represented are more than in case of higher sparsity, models use “superposition” here.\nHowever, there might be some interference as shown in 90% sparsity case — basically, a feature representation will have a non-zero component along another feature representation.\n\nModels can perform computation while in superposition.\n\nModels can put simple circuits computing the absolute value function in superposition. (What?)\nHypothesis: Neural networks observed in practice noisily simulate larger, highly sparse networks. — Models can be thought of as doing the same thing as an imagined much-larger model representing the exact same features but with no interference.\n\nFeature superposition in other places! (Read later)\n\nLinear Algebraic structure of word sense, with applications to polysemy (TACL 2018, Arora et. al)\nDecoding the Thought Vector (2016, G. Goh)\nZoom In: An Introduction to Circuits (Distill 2020, Olah et. al)\nSoftmax Linear Units (Transformer Circuits Thread 2022, Elhage et. al)\nCompressed Sensing (IEEE TIT 2006, Donoho et. al.)\nLocal vs. Distributed Coding (Intellectica 1989, Thorpe)\nRepresentation learning: A review and new perspectives (IEEE TPAMI 2013, Bengio et. al.)\n\nWhat do they show in this paper?\n\nInterpreting NNs as having sparse structure in superposition is not only a useful post-hoc interpretation but the “ground truth” of the model.\nWhen and why? → “phase” diagram for superposition.\n\nWhy neurons are sometimes monosemantic and sometimes polysemantic?\n\nMonosemantic → respond to a single feature.\nPolysemantic → respond to multiple features.\n\n\n\n\nSuperposition exhibits complex geometric structure based uniform polytopes.\n\nUniform polytopes\n\nClass of regular polytopes in any number of dimensions. Think regular polygons in 2D, regular polyhedra in 3D.\n*Conditions for uniformity\n\nVertex-transitivity: all vertices are equivalent under the symmetries of the polytope.\nEdge-transitivity: all edges are equivalent.\nFace-transitivity: all facets (faces) are equivalent.\nVertex figures are regular: Figure formed by the intersection of the polytope with its facets must be a regular polytope of one dim lower.\n\n\n\n\nPreliminary evidence that superposition may be linked to\n\nadversarial examples\ngrokking (?)\ntheory for the performance of mixture of experts model.\n\n\n\n\n\nDefinitions and Motivations: Features, Directions and Superposition\n\nThey think of features of inputs as directions in activation space. Like in the example above — there are 2 dim in activation space and 5 features in the input.\n“Linear Representation Hypothesis”.  Two separate properties\n\nDecomposability →  Network representations can be described in terms of independently understandable features.\nLinearity → Features are represented by direction.\n\n\n\nNotes from Neel Nanda’s Walkthrough Video\n\nHigh-dimensional spaces are not intuitive to understand.\n\nIt is easier to fit a lot of features (nearly orthogonal) in very high-dimensional spaces.\n\n\nInterference vs. Internal Representation. The linear map encoder projects input into 2 dimensions and read-out using dot product (basically projecting into some direction) → from 2-dim to 5-dim. Key point is that encoding and decoding are not inverse operations. Project down operation is not inverse of embed up.\nSuperposition in Toy models vs. Real transformers. Discussion on where transformer might be compressing. Residual stream has much lower dimensions (bottleneck dimensions). Non-linearities cause more interference.\nAbout non-privileged basis →\n\nFeatures are not invariant under a change of basis.\n\nWE​Win​ → (WE​R)(R⊤Win​) will change the internal representations.\nThis is a non-privileged basis.\n\n\n\n\nSome weird stuff!\n\nNeel says that neural networks in computers are computational objects not abstract linear algebra objects. So in computers, the way you represent vectors as floats in a computer is always privileged.\nFloating points represented in a computer — 32 bits (MSB - signed bit (+1 or -1), floating point representation in binary, exponent bit, another binary string).\n\n\n\nReferences\n[1] Elhage, et al., “Toy Models of Superposition”, Transformer Circuits Thread, 2022.\n[2] A Walkthrough of Toy Models of Superposition w/ Jess Smith - YouTube"},"Daily-Notes/index":{"title":"Daily Notes","links":[],"tags":[],"content":""},"Paper-Reading/How-Does-IB-help-DL":{"title":"How Does Information Bottleneck Help Deep Learning?","links":[],"tags":[],"content":"In the paper Representation compression and generalization in deep neural networks (Shwartz-Ziv et al., 2019), the following conjecture is given.\n\n\n                  \n                   Conjecture 1. (Informal Version)\n                  \n                \nWith probability 1−δ over the training data s={(xi​,ui​)}i=1n​ drawn from the same distribution as a random variable pair (X,Y), for the generalization error Δ(s)=EX,Y​[l(fs(X),Y)]−n1​∑i=1n​l(fs(xi​),yi​), there is a bound obeying the following form:\nΔ(s)⩽2n2I(X;Zls​)+logδ2​​​,\nwhere fs is the full model obtained by training and Zls​=ϕls​(X) is the output of an intermediate l-layer encoder ϕls​ of the model, i.e., representation obtained after passing through the first l layers.\n"},"Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge":{"title":"Large Language Models Struggle to Learn Long-Tail Knowledge","links":[],"tags":["llms","empirical"],"content":"In this paper, the authors show that a language model’s ability to answer fact-based questions is dependent on the related number of documents it pre-trained on. The general observation is that lesser the number of relevant (to the QA pair) documents the model pre-trained on, worse is its QA performance for that particular pair.\nMotivating Questions\n\nMany concepts or much knowledge appears rarely on the Internet (primary source for LLM training), are language models able to learn this knowledge well? How is the LLM’s ability to answer the question affected by the number of related documents seen during the pre-training?\nIf there is a correlation between the number of relevant documents and the QA performance, does it also mean causation?\nHow can such long-tail knowledge be better captured? How does model scaling and retrieval-augmentation affect performance on long-tail knowledge?\nWhat kind of knowledge language models capture − do they learn “easy” facts that appear more frequently in the pre-training data?\n\nHow?\n\n\n                  \n                  Question\n                  \n                \nHow do they find the relevant documents and count them?\nEntity linking the pre-training datasets and the counting documents with the same entities as the QA pair. DBpedia Spotlight Entity Linker is run at massively distributed scale and the entities are linked to DBpedia or Wikidata IDs using traditional entity linking methods. This lets us store the document indices for each entity.\nNext, the QA pair is entity linked and the entities from both the question and ground-truth answer is extracted. Once this is done, count the number of documents from the earlier pre-trained dataset entity linking for the extracted QA entities where both the question entity and the answer entity co-occur.\n\n\nReferences\n[1] Kandpal, Nikhil, et al. “Large language models struggle to learn long-tail knowledge.” International Conference on Machine Learning. PMLR, 2023."},"Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough":{"title":"Language Models for Text Classification: Is In-Context Learning Enough?","links":["Daily-Notes/Masked-Language-Modeling"],"tags":["llms","incontextlearning","textclassification"],"content":"Link: [2403.17661] Language Models for Text Classification: Is In-Context Learning Enough?\nThis paper explores how text generation models combined with prompting techniques (in-context learning) compare to fine-tuning for text classification i.e., the authors compare zero- and few-shot approaches of LLMs to fine-tuning smaller language models. They find that finetuning smaller and efficient (?) language models outperforms few-shot approaches in large language models for text classification.\nThis note will be quite verbose and at times, verbatim, since this is the first time I am reading literature on this topic.\nPrior Research\n\nStandard approach for supervised text classification → Finetune LMs such as BERT using an additional classifier head but this requires a lot of data to achieve SOTA results and hence is not suitable for classifications tasks where class imbalances and data sparsity is present (data is generally hard to come by or is imbalanced in specific domains). More data, better performance!\nAlternative approach is based on using autoregressive text generation models which can perform unseen tasks through the use of prompting (they have zero and few-shot capabilities). In the zero-shot setting, instruction fine-tuning leads to improvement.\nPrompt-engineering techniques have been used to improve generalization and for domain and task-specific improvements.\n\nText Classification\nThree main approaches to text classification exist −\n\nLinear Methods: FastText is a linear classification model which integrates a linear model with a rank constraint which allows sharing parameters among classes and features. It also integrates word embeddings which are average into text. Advantages: Dealing with out-of-vocabulary words and fine-grained distinctions between classes.\nFine-tuning Methods: Models like BERT and RoBERTa trained with masked language modeling (MLM) objective can be adapted for text-classification by using fine-tuning techniques based on adding a single classification layer onto the model. As discussed above, it requires a lot of data.\nText Generation Models:\n\nContinuing soon…\nAppendix\nMasked Language Modeling (MLM)\nSee Masked Language Modeling for more details."},"Paper-Reading/index":{"title":"Paper Reading","links":[],"tags":[],"content":""},"index":{"title":"Akash Sharma","links":[],"tags":[],"content":"Akash SharmaMS (Research) Student, EE @ IIT Madras\nI’m currently a master’s graduate student at Department of Electrical Engineering at Indian Institute of Technology Madras. I am advised by Dr. Mohanasankar Sivaprakasam and Keerthi Ram. I am also a part of the analytics team at Sudha Gopalakrishnan Brain Centre, IIT Madras. My broad research interests include fundamental deep learning (theory and application) for both vision and language tasks.\nContact\nI am always excited to discuss/collaborate on anything ML, math and computing. The best way to reach out to me is via email but you can also find me on other platforms, please see bottom of this page."}}