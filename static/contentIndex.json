{"Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse":{"title":"Linear Decoding and Deep Nets with Neural Collapse","links":[],"tags":["math","informationtheory"],"content":"XY Han, Prevalence of Neural Collapse during the terminal phase of deep learning train., 2021.10.05\nThe following notes are based on the seminar video by one of the original authors of the Neural Collapse paper.\nSimplex equiangular tight frame (ETF): Equinorm, Equiangle, Maximal Angle.\n[NC1] Variability collapse: within-class variable collapse (features collapse)\n[NC2] Class mean convergence to simplex ETF: within a space where number of vectors/classes is smaller than the dimension of the space. The trad ETF is within a setting a where there are more vectors than dimensions → the definition of the frame is that the smallest eigenvalue of the vectors of the frame is greater than zero, it spans the space. Here, the spanning condition is relaxed and the structure is retained.\n[NC3] Convergence to self-duality: means and classifier converge to each other. \n[NC4] Behavioral convergence to nearest class center: The classifier is simply the nearest neighbor to the class mean (See theoretical proof in the paper).\n\n\n                  \n                  What advantages do we have if we train beyond zero error? \n                  \n                \n\nImproves generalization, test performance.\nImproves adversarial robustness (DeepFool metric)\n\n\nFeature Engineering\nNotation exposes only last-layer objects: W and hi,c​=hi,cL​.\nFull notation (W,Θ)\n\nW : last-layer classifier (C-dimensional vectors)\nΘ : parameters of earlier layers\nLast-layer activations: \nhi,C​=h(Xi,c​;Θ)\n\nOptimizing h(⋅;Θ): ‘Feature engineering’.\n\n\n                  \n                  Neural Collapse is the preferred end-state of every successful exercise in feature engineering. Similar idea was proposed by Shannon in his 1959 paper “Probability of Error for Optimal Codes in a Gaussian Channel”.\n                  \n                \n\nLinear Decoding Over Gaussian Channel\nTransmitter transmits codeword: μγ​∈{μC​}c=1C​, γ∼Unif{1,...,C}.\nReceiver receives: h=μγ​+z, z∼N(0,σ2I).\nLinear decoder with weights  {wc​}c=1C​ and biases {bc​}c=1C​ :\nγ^​(h)=argcmax​⟨wc​,h⟩+bc​\nGoal: Design weights, biases and codewords such that ∣∣μc​∣∣2​⩽1. Here we have a norm bound on the means. So choose the best codeword so that the decoder’s job is the easiest, basically learn features in such a way that it is easier to classify as for our case.\nThis is similar to the feature engineering problem we have. Shannon gave bounds for what the angles between the codewords must be as the dimension of the codewords (size of the codewords) increases.\nFor us, we look at other things. We know that for deep nets, the signal overwhelms the noise therefore σ→0. So what is the limiting ratio that we should look at, limit exists as we go to infinity? Defined below. (this is different from what Shannon considered).\nLarge deviations error exponent →\nβ(H,W,b)=−σ2logPσ​{γ^​(h)=γ}\nwhere −σ2 and logPσ​{γ^​(h)=γ} are opposing as initially noise is high and signal is low, thus misclassification occurs and as we keep training σ lowers essentially tending to 0, first time thus increasing and the rate of misclassification decreases.\nOptimal Error Exponent →\nβC∗​≡H,W,bmax​β(H,W,b)\nReferences\n[1] Papyan, Vardan, X. Y. Han, and David L. Donoho. “Prevalence of neural collapse during the terminal phase of deep learning training.” Proceedings of the National Academy of Sciences 117.40 (2020): 24652-24663."},"Daily-Notes/Some-definitions-in-Information-Theory":{"title":"Some definitions in Information Theory","links":[],"tags":["math"],"content":"\nEntropy can intuitively be thought of as a measure of information, for any probability distribution or in other words, uncertainty of a random variable.\nMutual information is the measure of the amount of information one random variable contains about another.\nRelative Entropy is the measure of the distance between two probability distributions.\n\n\n\n                  \n                  Note\n                  \n                \nEntropy can be thought of as the self-information of a random variable and mutual information is a special case of relative entropy.\n\nEntropy\n\n\n                  \n                  Definition\n                  \n                \nThe entropy H(X) of a discrete random variable X is defined by\nH(X)=−x∈X∑​p(x)logp(x)\n\nJoint Entropy\n\n\n                  \n                  Definition\n                  \n                \nThe joint entropy H(X,Y) of a pair of discrete random variables (X,Y) with a joint distribution p(x,y) is defined as\nH(X,Y)=−x∈X∑​y∈Y∑​p(x,y)logp(x,y)\nalso,\nH(X,Y)=−Elogp(X,Y)\n\nRelative Entropy or Kullback-Leibler Distance\n\n\n                  \n                  Definition\n                  \n                \nThe relative entropy or Kullback-Leibler distance between two probability mass functions p(x) and q(x) is defined as\nD(p∣∣q)​=x∈X∑​p(x)logq(x)p(x)​=Ep​logq(X)p(X)​​\n\nMutual Information\n\n\n                  \n                  Definition\n                  \n                \nConsider two random variables X and Y with a joint probability mass function p(x,y) and marginal probability mass functions p(x) and p(y). The mutual information I(X,Y) is the relative entropy between the joint distribution and the product distribution p(x)p(y):\nI(X;Y)​=x∈X∑​y∈Y∑​p(x,y)logp(x)p(y)p(x,y)​=D(p(x,y)∣∣p(x)p(y))=Ep(x,y)​logp(X)p(Y)p(X,Y)​​\n"},"Daily-Notes/index":{"title":"Daily Notes","links":[],"tags":[],"content":""},"Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge":{"title":"LLMs Struggle to Learn Long-Tail Knowledge","links":[],"tags":["llms","empirical"],"content":"In this paper, the authors show that a language model’s ability to answer fact-based questions is dependent on the related number of documents it pre-trained on. The general observation is that lesser the number of relevant (to the QA pair) documents the model pre-trained on, worse is its QA performance for that particular pair.\nMotivating Questions\n\nMany concepts or much knowledge appears rarely on the Internet (primary source for LLM training), are language models able to learn this knowledge well? How is the LLM’s ability to answer the question affected by the number of related documents seen during the pre-training?\nIf there is a correlation between the number of relevant documents and the QA performance, does it also mean causation?\nHow can such long-tail knowledge be better captured? How does model scaling and retrieval-augmentation affect performance on long-tail knowledge?\nWhat kind of knowledge language models capture − do they learn “easy” facts that appear more frequently in the pre-training data?\n\nHow?\n\n\n                  \n                  Question\n                  \n                \nHow do they find the relevant documents and count them?\nEntity linking the pre-training datasets and the counting documents with the same entities as the QA pair. DBpedia Spotlight Entity Linker is run at massively distributed scale and the entities are linked to DBpedia or Wikidata IDs using traditional entity linking methods. This lets us store the document indices for each entity.\nNext, the QA pair is entity linked and the entities from both the question and ground-truth answer is extracted. Once this is done, count the number of documents from the earlier pre-trained dataset entity linking for the extracted QA entities where both the question entity and the answer entity co-occur.\n\n\nReferences\n[1] Kandpal, Nikhil, et al. “Large language models struggle to learn long-tail knowledge.” International Conference on Machine Learning. PMLR, 2023."},"Paper-Reading/index":{"title":"Paper Reading","links":[],"tags":[],"content":""},"index":{"title":"Akash Sharma","links":[],"tags":[],"content":"Akash SharmaMS (Research) Student, EE @ IIT Madras\nI’m currently a master’s graduate student at Department of Electrical Engineering at Indian Institute of Technology Madras. I am advised by Dr. Mohanasankar Sivaprakasam and Keerthi Ram. I am also a part of the analytics team at Sudha Gopalakrishnan Brain Centre, IIT Madras. My broad research interests include fundamental deep learning (theory and application) for both vision and language tasks.\nContact\nI am always excited to discuss/collaborate on anything ML, math and computing. The best way to reach out to me is via email but you can also find me on other platforms, please see bottom of this page."}}