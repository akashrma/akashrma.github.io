<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
    <channel>
      <title>Akash Sharma</title>
      <link>https://akash7243.github.io/homepage</link>
      <description>Last 10 notes on Akash Sharma</description>
      <generator>Quartz -- quartz.jzhao.xyz</generator>
      <item>
    <title>A note on a note on mechanistic interpretability, variables, and importance of interpretable bases</title>
    <link>https://akash7243.github.io/homepage/Daily-Notes/Mech-Interp,-Variables,-Interpretable-Bases</link>
    <guid>https://akash7243.github.io/homepage/Daily-Notes/Mech-Interp,-Variables,-Interpretable-Bases</guid>
    <description>These notes are notes on Chris Olah’s Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases :) Olah presents an analogy between regular computer ...</description>
    <pubDate>Tue, 16 Jul 2024 21:05:09 GMT</pubDate>
  </item><item>
    <title>Notes on Toy Models of Superposition</title>
    <link>https://akash7243.github.io/homepage/Daily-Notes/Toy-Models-of-Superposition</link>
    <guid>https://akash7243.github.io/homepage/Daily-Notes/Toy-Models-of-Superposition</guid>
    <description>Notes from the original paper Toy models used — small ReLU networks trained on synthetic data with sparse input features. What do they mean by “sparse” features? Most of the values in the feature space are zero.</description>
    <pubDate>Tue, 16 Jul 2024 18:14:47 GMT</pubDate>
  </item><item>
    <title>How Does Information Bottleneck Help Deep Learning?</title>
    <link>https://akash7243.github.io/homepage/Paper-Reading/How-Does-IB-help-DL</link>
    <guid>https://akash7243.github.io/homepage/Paper-Reading/How-Does-IB-help-DL</guid>
    <description>In the paper Representation compression and generalization in deep neural networks (Shwartz-Ziv et al., 2019), the following conjecture is given. Conjecture 1.</description>
    <pubDate>Tue, 09 Jul 2024 12:58:54 GMT</pubDate>
  </item><item>
    <title>Masked Language Modeling</title>
    <link>https://akash7243.github.io/homepage/Daily-Notes/Masked-Language-Modeling</link>
    <guid>https://akash7243.github.io/homepage/Daily-Notes/Masked-Language-Modeling</guid>
    <description>Setup With BERT, the input sequence is a concatenation of two sequences of tokens (x1​,...,xN​) and (y1​,...,yM​), this input sequence is presented as follows: [CLS],x1​,...,xN​,[SEP],y1​,...,yM​,[EOS] ...</description>
    <pubDate>Fri, 28 Jun 2024 22:24:42 GMT</pubDate>
  </item><item>
    <title>Language Models for Text Classification: Is In-Context Learning Enough?</title>
    <link>https://akash7243.github.io/homepage/Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough</link>
    <guid>https://akash7243.github.io/homepage/Paper-Reading/Language-Models-for-Text-Classification---Is-ICL-Enough</guid>
    <description>Link: [2403.17661] Language Models for Text Classification: Is In-Context Learning Enough? This paper explores how text generation models combined with prompting techniques ...</description>
    <pubDate>Fri, 28 Jun 2024 19:14:55 GMT</pubDate>
  </item><item>
    <title>Linear Decoding and Deep Nets with Neural Collapse</title>
    <link>https://akash7243.github.io/homepage/Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse</link>
    <guid>https://akash7243.github.io/homepage/Daily-Notes/Linear-Decoding-and-Deep-Nets-with-Neural-Collapse</guid>
    <description>XY Han, Prevalence of Neural Collapse during the terminal phase of deep learning train., 2021.10.05 The following notes are based on the seminar video by one of the original authors ...</description>
    <pubDate>Thu, 27 Jun 2024 19:19:48 GMT</pubDate>
  </item><item>
    <title>Some definitions in Information Theory</title>
    <link>https://akash7243.github.io/homepage/Daily-Notes/Some-definitions-in-Information-Theory</link>
    <guid>https://akash7243.github.io/homepage/Daily-Notes/Some-definitions-in-Information-Theory</guid>
    <description>...</description>
    <pubDate>Mon, 24 Jun 2024 19:37:06 GMT</pubDate>
  </item><item>
    <title>Large Language Models Struggle to Learn Long-Tail Knowledge</title>
    <link>https://akash7243.github.io/homepage/Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge</link>
    <guid>https://akash7243.github.io/homepage/Paper-Reading/LLMs-Struggle-to-Learn-Long-Tail-Knowledge</guid>
    <description>In this paper, the authors show that a language model’s ability to answer fact-based questions is dependent on the related number of documents it pre-trained on ...</description>
    <pubDate>Mon, 17 Jun 2024 11:56:53 GMT</pubDate>
  </item><item>
    <title>Paper Reading</title>
    <link>https://akash7243.github.io/homepage/Paper-Reading/</link>
    <guid>https://akash7243.github.io/homepage/Paper-Reading/</guid>
    <description></description>
    <pubDate>Mon, 17 Jun 2024 11:56:30 GMT</pubDate>
  </item><item>
    <title>Daily Notes</title>
    <link>https://akash7243.github.io/homepage/Daily-Notes/</link>
    <guid>https://akash7243.github.io/homepage/Daily-Notes/</guid>
    <description></description>
    <pubDate>Wed, 12 Jun 2024 15:57:09 GMT</pubDate>
  </item>
    </channel>
  </rss>